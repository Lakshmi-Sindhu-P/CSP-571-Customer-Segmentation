---
title: "Customer Segmentation and Predicting Lifetime Value in E-Commerce"
output: html_notebook: default

  html_document: default
---

### **Data Overview - **
Note - The information provided is from the official page of the dataset, as mentioned in the Source of data part.

* <b>Source of data -</b> <a href = "https://archive.ics.uci.edu/dataset/502/online+retail+ii"> Online Retail II </a>from  UCI Machine Learning Repository, Focusing on data from Sheet-Year 2010-2011
* <b>Dataset Information -</b> This Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers. It consists of Missing Values.
* <b>Description of Feautres -</b>
  1. <i><b>InvoiceNo:</i></b> Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation. 
  2. <i><b>StockCode:</i></b> Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product. 
  3. <i><b>Description:</i></b> Product (item) name. Nominal. 
  4. <i><b>Quantity:</i></b> The quantities of each product (item) per transaction. Numeric.	
  5. <i><b>InvoiceDate:</i></b> Invice date and time. Numeric. The day and time when a transaction was generated. 
  6. <i><b>UnitPrice:</i></b> Unit price. Numeric. Product price per unit in sterling (Â£). 
  7. <i><b>CustomerID:</i></b> Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.
  8. <i><b>Country:</i></b> Country name. Nominal. The name of the country where a customer resides.


<h3>**Project Requirements - **</h3>
```{r}
# Installing Packages
# install.packages("readxl")
install.packages("highcharter")

```

```{r}
# Suppress warning messages
options(warn = -1)

# Loading Libraries
library("readxl") #Data Loading
library(dplyr) #Data Manipulation (Counting Distinct Values, Renaming Column Names)
library(naniar) #Visualising Missing Plots
library(tidyr) #Data Transformation(Handling Missing Values)
library(dlookr) #Visualising Outliers
library(lubridate) #Feature Engineering
library(DT) #Interactive Data Table

library(highcharter) #EDA
library(ggplot2) #EDA
library(xts) #Creating and manipulating time series objects(EDA)
library(wordcloud2) #EDA
library(wordcloud) #EDA
library(tm) #EDA
library(RColorBrewer) #EDA
library(countrycode) #EDA
library(viridisLite) #EDA

library(stringr) #RFM 
library(htmltools) #Plot
library(highcharter) #Plot

library(factoextra) #Clustering
library(cluster) #Clustering
library(NbClust) #Determining Best Number of Clusters

```


```{r}
# Loading dataset and create copy to work on
my_data <- read_excel("online_retail_II.xlsx", sheet = "Year 2010-2011")
Cust_Seg <- my_data

```

<h3>**Quick Look Into The Data - **</h3>
<h4>Checking the internal structure of the data:</h4>

```{r}
str(Cust_Seg)
```

```{r}
# Printing Column Names
names(Cust_Seg)

#View of first few rows of Data
head(Cust_Seg)
```
**Observations - **
  
  1. The data consists of <b>541,909 rows and 8 columns/features</b>, namely - 
  "`Invoice`", "`StockCode`" ,"`Description`", "`Quantity`", "`InvoiceDate`", "`Price`", "`Customer ID`", "`Country`".
  2. We can also see that the values of `Invoice`, which are suppose to be num,
  are categorised as character. Also the `InvoiceDate` feature values are in a bad
  format, which we will have to change during the data cleaning part.
  3. Also, we can observe that the column names consist of spaces, which may cause
  errors in further analysis. Hence, we will also change the column names to make
  it easy in the future steps.

<br>
<h4>Summary of the data:</h4>

```{r}
summary(Cust_Seg)
```
**Observations - **

From the summary of the dataset, we gather the following insights about its various features:

1. **`Invoice`**: The dataset contains 541,909 unique invoice numbers represented as character strings.
2. **`StockCode`**: There are 541,909 unique stock codes also represented as character strings.
3. **`Description`**: The dataset has 541,909 unique descriptions for the products, represented as character strings.
4. **`Quantity`**: The minimum value is -80,995, indicating potentially incorrect or cancelled orders. The mean quantity is approximately 9.55, with a wide range of values from -80,995 to 80,995.
5. **`InvoiceDate`**: Transactions span from December 1, 2010, to December 9, 2011, covering slightly over a year.
6. **`Price`**: The minimum unit price is -11,062.06, which seems to be an error or a credit note. The mean unit price is around 4.61, with values ranging up to 38,970.
7. **`Customer ID`**: There are 135,080 missing values (NA) for customer IDs. The remaining values range from 12,346 to 18,287.
8. **`Country`**: The dataset includes transactions from multiple countries, with each country represented as a character string.

**Additional Observations**:

- The negative values in `Quantity` and `Price` suggest potential cancelled orders or errors that need to be addressed during the data cleaning phase.
- The dataset spans slightly over a year, from December 1, 2010, to December 9, 2011, providing a comprehensive view of transactions during this period.

<br>

Before finding the unique products, let's do a part of the data manipulation process here as per the description of the official website we got our data from, that is - **`InvoiceNo`, `StockCode`, `Description`, `Quantity`, `InvoiceDate`, `UnitPrice`, `CustomerID`, `Country`.**
<h4>Changing Column Names:</h4>

```{r}
colnames(Cust_Seg) <- c('InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country')
```

```{r}
names(Cust_Seg)
```
Now that we have successfully changed the column names, let's continue our data exploration.

<h4>Unique customers & products:</h4>

```{r}
n_distinct(Cust_Seg$CustomerID)
n_distinct(Cust_Seg$Description)

```
**Observations - **<br>
We can observe that we have 4,373 unique Customer IDs(indicating 4,373 customers) and 4,212 Unique descriptions(indicating 4,212 products).


<h3>**Data Pre-Processing - **</h3>
<h4>Data Cleaning: </h4>
  1. <u>Checking and dealing with missing values</u>
```{r}
print(paste("Count of total missing values - ", sum(is.na(Cust_Seg))))

```
```{r}
# count the missing values by column wise
sapply(Cust_Seg, function(x) sum(is.na(x)))
```

```{r}
#Plot of the missing values(In terms of %) - 
gg_miss_var(Cust_Seg, show_pct = TRUE)

```
  **Observations - **<br>
      We can see that the total number of missing values in the dataset are 136534, of which 1454 are from the `Description` column while the remaining 135080 are from `CustomerID` column.
      Based on this,

  **1. CustomerID Missing Values:**
  A significant number of `CustomerID` values are missing from the dataset. Since the dataset is extensive with a large sample size, it's reasonable to remove these missing values. Keeping these missing values could potentially lead to inaccurate or biased results in the analysis.<br>
  **2. Description Missing Values:**
A small percentage of Description values are missing. These missing descriptions will be replaced with an empty string to maintain data integrity and completeness.


Based on these observations,
<h5> Handling Missing Values-</h5>
```{r}
# Deleting null CustomerID rows
Cust_Seg <- Cust_Seg %>% 
  na.omit(Cust_Seg$CustomerID)

# Replacing NA Description values with the string saying "empty"
Cust_Seg$Description <- replace_na(Cust_Seg$Description, "empty")
```

```{r}
# Checking for null values column wise post Handling.
print("Count of missing values by column wise")
sapply(Cust_Seg, function(x) sum(is.na(x)))
```
Now that we have gotten rid of the null values, let's check for Outliers in our data.

  2. <u>Checking and Handling Outliers of the data</u>
  
  For detecting outliers we will go with a simple package- `dlookr` which could be one of the best ways for outlier handling, which is a very essential step in data cleaning and pre-processing.
  
```{r}
# Plotting Outliers for the features - Quantity and UnitPrice
plot_outlier(Cust_Seg, Quantity, UnitPrice, col = "#db7093")

```

Lets check the minimum and maximum Unit Price of products,

```{r}
print(paste("Minumum Unit Price -" , min(Cust_Seg$UnitPrice)))
print(paste("Maximum Unit Price -" , max(Cust_Seg$UnitPrice)))
```
**Observations - **

- **Negative and Zero Values in Quantity**:
  - The dataset contains negative and zero values in the `Quantity` column.
  - These values need to be removed as they may represent cancelled orders, returns, or data errors.

- **Connection Between Positive and Negative Outliers**:
  - Positive outliers in the `Quantity` data appear to be connected to negative outliers.
  - Some positive outliers might be cancelled or returned orders, resulting in negative values.
  - Both positive and negative outliers that don't represent actual orders should be removed to avoid data bias.

- **High Unit Price Values**:
  - The dataset includes extremely high unit prices, with the maximum value being 38,970.
  - These high-value items may require further investigation to ensure data accuracy and relevance.

Let's check the connection between the positive and negative outliers - 
```{r}
# checking quantities that are negative, arranged by descending order
quantity_check <- Cust_Seg %>% 
  filter(Quantity < 0) %>% 
  arrange(Quantity)

head(quantity_check)
```
**Observations - **

- **Negative Quantities**:
  - The code identifies negative `Quantity` values, indicating cancelled orders or returns in the `Cust_Seg` dataset.
  - The data is sorted in descending order based on `Quantity` to highlight extreme cases.

- **Cancelled Orders**:
  - The negative quantities correspond to cancelled orders, as indicated by the "C" prefix in Invoice Numbers.
  - There are 8,905 cancelled orders in total, with some orders having exceptionally large quantities.

- **Specific Customer Inquiry**:
  - A specific customer with ID 16446 had a cancelled order for 80,995 units of "Paper Craft, Little Birdie" products.
  - Such extreme quantities require further investigation to ensure data accuracy and understand the reasons behind such large cancellations.

Let's Check the customer with ID 16446 - 

```{r}
Cust_Seg %>% 
  filter(CustomerID == 16446)
```
**Observations - **

- Customer 16446 placed an order for 80,995 units of "Paper Craft, Little Birdie" but cancelled it shortly after.
- Both the original order and its cancellation (negative quantity) are present in the dataset.
- Removing only the negative quantity might distort the analysis.
- It's crucial to remove both the positive and negative counterparts of cancelled orders to maintain data accuracy.

- **Specific Customer Insights**:
  - Customer 16446 and 12346 have orders with both positive and negative quantities, indicating cancellations.
  - These orders need to be removed to avoid bias in the analysis.

We will proceed to remove the positive counterparts of cancelled orders for Customers 16446 and 12346.
We will also review other customers with large cancelled orders, like Customer 15749, to ensure data consistency and integrity.

```{r}
# deleting the outliers by their InvoiceNo
Cust_Seg <- Cust_Seg[!(Cust_Seg$InvoiceNo == 581483 | Cust_Seg$InvoiceNo == 541431),]
```

Now we will clean the dataset by filtering out negative and zero values of the Quantity and UnitPrice columns by checking the outlier plots again with the clean filtered dataset- 
```{r}
# filtering data for positive Quantities only
Cust_Seg <- Cust_Seg %>% 
  filter(Quantity > 0) %>% 
  filter(UnitPrice >0)

plot_outlier(Cust_Seg, Quantity, UnitPrice, col = "#db7093")
```

**Observations - **
After filtering out negative and zero values from the `Quantity` and `UnitPrice` columns, the cleaned dataset shows more accurate outlier plots. Most products are priced between 0 to 4 currency units, highlighting a focus on lower-priced items. This cleaned dataset can now be used for further analysis to uncover meaningful insights.


Now, let's check the unique countries that the dataset contains the transaction data from:
```{r}
unique(Cust_Seg$Country)
```
We can see that there are 37 countries from where the data has been extracted. Now that our data is clean, let's move forward with the Feature engineering for our analysis.

<h4>Feature Engineering: </h4>
What we will be doing in this section - 

1. We will create a new column named "Spent" by multiplying `Quantity` with`UnitPrice` to calculate the total money spent on each product.
2. We will be generating a new customer dataframe by grouping it based on `CustomerID` to analyze the total amount spent by each customer.
3. We will extract separate date and time features from the existing `InvoiceDate` column to aid our analysis.
4. We will derive additional features like month, year, and hour after splitting the date and time to gather more insights.
5. We will also adjust the date format for consistency across the dataset.
6. We will be introducing another new feature to determine the day of the week.
7. Finally we will set up a unique descriptions frame for exploring the products offered.

```{r}
# Creating a new column "Spent" to calculate the total amount spent on each product
Cust_Seg <- mutate(Cust_Seg, Spent = Quantity * UnitPrice)

# Grouping by CustomerID and Country to summarize total spending and quantity
customer <- summarise_at(group_by(Cust_Seg, CustomerID, Country), vars(Spent, Quantity), list(sum = ~ sum(., na.rm = TRUE)))

# Extracting separate date and time features from InvoiceDate
Cust_Seg$InvoiceDate <- as.character(Cust_Seg$InvoiceDate)
Cust_Seg$date <- sapply(Cust_Seg$InvoiceDate, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][1]})
Cust_Seg$time <- sapply(Cust_Seg$InvoiceDate, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][2]})

# Creating new columns for year, month, and hour
Cust_Seg$year <- sapply(Cust_Seg$date, FUN = function(x) {strsplit(x, split = '[-]')[[1]][1]})
Cust_Seg$month <- sapply(Cust_Seg$date, FUN = function(x) {strsplit(x, split = '[-]')[[1]][2]})
Cust_Seg$hour <- sapply(Cust_Seg$time, FUN = function(x) {strsplit(x, split = '[:]')[[1]][1]})

# Converting date to Date type
Cust_Seg$date <- as.Date(Cust_Seg$date, "%Y-%m-%d")

# Creating day of the week feature
Cust_Seg$day_week <- wday(Cust_Seg$date, label = TRUE)

# Creating a unique products list for further exploration
products_list <- unique(Cust_Seg$Description)

```

Let's have a look at the structure of the dataset post these additions - 

```{r}
head(Cust_Seg, 3)
```
Now we see the InvoiceNo column consists of the Character - "C" to indicate canceled orders, making the datatype of this column accurate, giving no reason to change it.
We will now create an Interactive customer data table, where we will be able to filter the different columns to find information about specific fields, sort the features and search for values and/or ranges.

```{r}
# Creation of interactive customer data table
datatable(
  customer, 
  class = 'cell-border stripe', 
  filter = 'top', 
  options = list(
    pageLength = 10, 
    autoWidth = TRUE, 
    columnDefs = list(list(className = 'dt-center', targets = "_all")), 
    searchHighlight = TRUE
  ),
  caption = htmltools::tags$caption(
    style = 'caption-side: bottom; text-align: center;',
    'Table 1: ', htmltools::em('Customer datatable Information ')
  )
)

```
Which marks the end of the Data Pre-Processing Part of our project.

The next step is EDA.

<h3>**Explorqatory Data Analysis(EDA) - **</h3>

```{r}
# Grouping by Country and summarizing the number of transactions
plot_clean1 <- Cust_Seg %>% 
  group_by(Country) %>% 
  dplyr::summarise(n = n()) 

# Creating a column chart using highcharter
highchart() %>% 
  hc_chart(type ="column",
           options3d = list(enabled = TRUE, alpha = 15, beta = 15)) %>%
  hc_xAxis(categories = plot_clean1$Country) %>% 
  hc_add_series(data = plot_clean1$n, name = "Total Invoices", color = '#83429a') %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_title(
    text="Total Invoices - Transaction per Country"
    ) %>%
  hc_chart(
    borderColor = '#EBBA95',
    borderRadius = 10,
    borderWidth = 1,
    backgroundColor = list(
      linearGradient = c(0, 0, 500, 500), stops = list(
        list(0, '#db7093'),
        list(0.2, '#c58fda'),
        list(0.4, '#abb6db'),
        list(0.6, '#c5e0db'),
        list(0.8, '#dde7db'),
        list(1, '#f2f1ef')
      ))
  )

```


```{r}
# Grouping by Country and summarizing the number of transactions excluding the United Kingdom
plot_clean <- Cust_Seg %>% 
  group_by(Country) %>% 
  filter(Country != "United Kingdom") %>% 
  dplyr::summarise(n = n()) %>% 
  arrange(-n)

# Creating a column chart using highcharter
highchart() %>% 
  hc_chart(type ="column",
           options3d = list(enabled = TRUE, alpha = 15, beta = 15)) %>%
  hc_xAxis(categories = plot_clean$Country) %>% 
  hc_add_series(data = plot_clean$n, name = "Total Invoices", color = '#c5cbe1') %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_title(
    text="Total Invoices - Transaction per Country excl. UK (descending)"
    ) %>%
  hc_chart(
    borderColor = '#EBBA95',
    borderRadius = 10,
    borderWidth = 1,
    backgroundColor = list(
      linearGradient = c(0, 0, 500, 500), stops = list(
        list(0, '#c6d8e0'),
        list(0.2, '#c6e0db'),
        list(0.4, '#c7dfcf'),
        list(0.6, '#ccdec8'),
        list(0.8, '#d6dec8'),
        list(1, '#dddac9')
      ))
  )

```


```{r}
# Grouping by Country and summarizing revenue and transactions
retail_country <- Cust_Seg %>%
  group_by(Country) %>%
  dplyr::summarise(revenue = sum(Spent), transactions = n_distinct(InvoiceNo)) %>%
  mutate(aveOrdVal = (round((revenue / transactions), 2))) %>%
  ungroup() %>%
  arrange(desc(revenue))

# Filtering top countries
top_countries_filter <- Cust_Seg %>%
  filter(Country == 'Netherlands' | Country == 'EIRE' | Country == 'Germany' | Country == 'France' 
         | Country == 'Australia')

# Summarizing top 5 countries by revenue
top_5 <- top_countries_filter %>%
  group_by(Country, date) %>%
  dplyr::summarise(revenue = sum(Spent), transactions = n_distinct(InvoiceNo), 
                   customers = n_distinct(CustomerID)) %>%
  mutate(aveOrdVal = (round((revenue / transactions), 2))) %>%
  ungroup() %>%
  arrange(desc(revenue))

# Creating a treemap chart for top 5 countries by revenue
top_5 %>% 
  group_by(Country) %>%
  dplyr::summarise(revenue = sum(revenue)) %>% 
  hchart('treemap', hcaes(x = 'Country', value = 'revenue', color = 'revenue')) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_title(text="Top 5 Countries by Revenue excl. UK")

```


```{r}
ggplot(top_5, aes(x = date, y = revenue, colour = Country)) + 
  geom_smooth(method = 'auto', se = FALSE, aes(fill = Country)) + 
  labs(x = 'Country', y = 'Revenue', title = 'Revenue by Country over Time') + 
  theme(panel.grid.major = element_line(colour = NA),
        legend.text = element_text(colour = "#a54fc4"),
        legend.title = element_text(face = "bold", colour = "#a54fc4"),
        panel.background = element_rect(fill = "seashell1", colour = NA),
        plot.background = element_rect(fill = "#c5cbe1", colour = NA),
        legend.key = element_rect(fill = "#c2ddf0"),
        legend.background = element_rect(fill = NA))

```


```{r}
# Calculating revenue by date
revenue_date1 <- Cust_Seg %>%
  group_by(date) %>%
  dplyr::summarise(revenue = sum(Spent))

# Creating a time series object
time_series <- xts(
  revenue_date1$revenue, order.by = as.POSIXct(revenue_date1$date))

# Plotting the time series using highcharter
highchart(type = "stock") %>% 
  hc_title(text = "Revenue by Date", style = list(color = "#f73788")) %>% 
  hc_subtitle(text = "Revenue generated from the online store", style = list(color = "#c5cbe1")) %>% 
  hc_add_series(time_series, name = "Revenue", color = "#a54fc4") %>%
  hc_add_theme(
    hc_theme_darkunica(
      chart = list(
        backgroundColor = "#1a1a1a"
      ),
      colors = c("#f73788", "#c5cbe1", "#a54fc4")
    )
  )



```


```{r}
# Revenue by the different day of the week
Cust_Seg %>%
  group_by(day_week) %>%
  dplyr::summarise(revenue = sum(Spent)) %>%
  hchart(type = 'column', hcaes(x = day_week, y = revenue)) %>% 
  hc_yAxis(title = list(text = "Revenue")) %>%  
  hc_xAxis(title = list(text = "Day of the Week")) %>% 
  hc_title(text = "Revenue by Day of Week") %>%
  hc_colors(c("#a54fc4"))

```


```{r}
# Revenue and transactions by Hour of the Day
Cust_Seg %>%
  group_by(hour) %>%
  dplyr::summarise(revenue = sum(Spent)) %>%
  hchart(type = 'column', hcaes(x = hour, y = revenue)) %>% 
  hc_yAxis(title = list(text = "Revenue")) %>%  
  hc_xAxis(title = list(text = "Hour Of Day")) %>% 
  hc_title(text = "Revenue by Hour Of Day") %>%
  hc_colors(c("#f73788"))

#Transactions by Hour of the Day
Cust_Seg %>%
  group_by(hour) %>%
  dplyr::summarise(transactions = n_distinct(InvoiceNo)) %>%
  hchart(type = 'column', hcaes(x = hour, y = transactions)) %>% 
  hc_yAxis(title = list(text = "Number of Transactions")) %>%  
  hc_xAxis(title = list(text = "Hour Of Day")) %>% 
  hc_title(text = "Transactions by Hour Of Day") %>%
  hc_colors(c("#3f4046"))
```


```{r}
# Total invoices per country excluding the UK
plot_clean <- Cust_Seg %>% 
  group_by(Country) %>% 
  filter(Country != "United Kingdom") %>% 
  dplyr::summarise(n = n()) %>% 
  arrange(-n)

highchart() %>% 
  hc_chart(type ="column") %>%
  hc_xAxis(categories = plot_clean$Country) %>% 
  hc_add_series(data = plot_clean$n, name = "Total Invoices") %>%
  hc_yAxis(title = list(text = "Total Invoices")) %>%
  hc_title(text="Total Invoices - Transaction per Country excl. UK (descending)") %>%
  hc_colors(c("#db7093"))
```


```{r}
# Visualizing top countries by revenue
retail_country <- Cust_Seg %>%
  group_by(Country) %>%
  dplyr::summarise(revenue = sum(Spent)) %>%
  arrange(desc(revenue))

top_5_countries <- retail_country %>% 
  slice_head(n = 5)

highchart() %>% 
  hc_chart(type ="column") %>%
  hc_xAxis(categories = top_5_countries$Country) %>% 
  hc_add_series(data = top_5_countries$revenue, name = "Total Revenue") %>%
  hc_yAxis(title = list(text = "Total Revenue")) %>%
  hc_title(text="Top 5 Countries by Revenue") %>%
  hc_colors(c("#c58fda"))
```


```{r}
# Wordcloud
topic.corpus <- Corpus(VectorSource(as.character(customer$Country)))

removeHTML <- function(text){
  text <- gsub(pattern = '<.+\\">', "", text)
  text <- gsub(pattern = '</.+>', "", text)
  return(text)
}

suppressWarnings({
  topic.corpus <- topic.corpus %>% 
    tm_map(content_transformer(removeHTML)) %>%
    tm_map(function(x) ifelse(nchar(x) > 0, x, NA)) %>%
    tm_map(removeNumbers) %>%
    tm_map(function(x) ifelse(nchar(x) > 0, x, NA)) %>%
    tm_map(removePunctuation) %>%
    tm_map(function(x) ifelse(nchar(x) > 0, x, NA)) %>%
    tm_map(stripWhitespace) %>%
    tm_map(function(x) ifelse(nchar(x) > 0, x, NA)) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(function(x) ifelse(nchar(x) > 0, x, NA)) %>%
    tm_map(removeWords, stopwords("english")) %>%
    tm_map(function(x) ifelse(nchar(x) > 0, x, NA)) %>%
    tm_map(removeWords, stopwords("SMART")) %>%
    tm_map(function(x) ifelse(nchar(x) > 0, x, NA))
})

# Remove NA values
topic.corpus <- topic.corpus[!is.na(as.character(topic.corpus))]

# Convert corpus to term-document matrix
tdm <- TermDocumentMatrix(topic.corpus)
m <- as.matrix(tdm)
word_freq <- sort(rowSums(m), decreasing = TRUE)

# Create a data frame
df <- data.frame(word = names(word_freq), freq = word_freq)

# Filter out single character words and some common terms
df <- df %>%
  filter(nchar(as.character(word)) > 1,
         word != "united", word != "kingdom")

# Define the colors and background
uxc.colors <- c("#1F77B4", "#FF7F0E", "#2CA02C", "#D62728", "#9467BD")
uxc.background <- "#f7e7ce"

# Generate the wordcloud
wordcloud2(df,
           color = rep_len(uxc.colors, nrow(df)),
           backgroundColor = uxc.background,
           fontFamily = "DM Sans",
           size = 1,
           minSize = 3,
           rotateRatio = 0)

```


```{r}
# preparing and cleaning the text
docs <- Corpus(VectorSource(products_list))

suppressWarnings({
  toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
  docs <- tm_map(docs, toSpace, "/")
  docs <- tm_map(docs, toSpace, "@")
  docs <- tm_map(docs, toSpace, "\\|")
  
  # Convert the text to lower case
  docs <- tm_map(docs, content_transformer(tolower))
  
  # Remove numbers
  docs <- tm_map(docs, removeNumbers)
  
  # Remove english common stopwords
  docs <- tm_map(docs, removeWords, stopwords("english"))
  
  # Remove punctuations
  docs <- tm_map(docs, removePunctuation)
  
  # Eliminate extra white spaces
  docs <- tm_map(docs, stripWhitespace)
})

# Create a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq = v)

# Generate word cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words = 20, random.order = FALSE, rot.per = 0.35, 
          colors = brewer.pal(8, "Dark2"))

```


```{r}
# Preparing and cleaning the text
docs <- Corpus(VectorSource(products_list))

suppressWarnings({
  toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
  docs <- tm_map(docs, toSpace, "/")
  docs <- tm_map(docs, toSpace, "@")
  docs <- tm_map(docs, toSpace, "\\|")
  
  # Convert the text to lower case
  docs <- tm_map(docs, content_transformer(tolower))
  
  # Remove numbers
  docs <- tm_map(docs, removeNumbers)
  
  # Remove English common stopwords
  docs <- tm_map(docs, removeWords, stopwords("english"))
  
  # Remove your own stop words
  docs <- tm_map(docs, removeWords, c("pink", "blue", "red", "set", "white", "metal", "glass", "large", "small", "holder"))
  
  # Remove punctuations
  docs <- tm_map(docs, removePunctuation)
  
  # Eliminate extra white spaces
  docs <- tm_map(docs, stripWhitespace)
})

# Reset warning options
options(warn = 0)

# Create a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq = v)

# Generate word cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words = 20, random.order = FALSE, rot.per = 0.35, 
          colors = brewer.pal(8, "Dark2"))

```

```{r}
# Grouping by Country and summarizing the number of transactions
map_info <- Cust_Seg %>% 
  group_by(Country) %>% 
  dplyr::summarise(revenue = sum(Spent))

# Creating the map visualization
highchart(type = "map") %>%
  hc_add_series_map(worldgeojson,
                    map_info %>% 
                      bind_cols(as_tibble(map_info$revenue)) %>% 
                      group_by(Country) %>% 
                      dplyr::summarise(revenue = log1p(sum(value))) %>% 
                      ungroup() %>% 
                      mutate(iso2 = countrycode(sourcevar = Country, 
                                               origin="country.name", destination="iso2c")),
                    value = "revenue", joinBy = "iso2") %>%
  hc_title(text = "Revenue by country (log)") %>%
  hc_tooltip(useHTML = TRUE, headerFormat = "",
             pointFormat = "{point.Country}: {point.revenue}") %>% 
  hc_colorAxis(stops = color_stops(colors = viridisLite::inferno(10, begin = 0.1)))

```

<h3>**Segmentation - Clustering - **</h3>
<h4>RFM feature engineering:</h4>
1. Recency
```{r}
# Calculating recency for each customer
recency <- Cust_Seg %>% 
  dplyr::select(CustomerID, InvoiceDate) %>% 
  mutate(recency = as.Date("2011-12-09") - as.Date(InvoiceDate, format="%Y-%m-%d"))

# Identifying the most recent purchase date for each customer
recency <- recency %>% 
  dplyr::select(CustomerID, recency) %>% 
  group_by(CustomerID) %>% 
  slice(which.min(recency))

# Displaying the first 3 rows
head(recency, 3)

```

2. Frequency - 
```{r}
# Calculateing the number of products purchased by each customer on each invoice date
amount_products <- Cust_Seg %>%
  dplyr::select(CustomerID, InvoiceDate) %>% 
  group_by(CustomerID, InvoiceDate) %>% 
  summarize(n_prod = n())

# Calculating the frequency of purchases for each customer
df_frequency <- amount_products %>% 
  dplyr::select(CustomerID) %>%
  group_by(CustomerID) %>% 
  summarize(frequency = n())

# Displaying the first 3 rows
head(df_frequency, 3)

```
3. Monetary Value - 

```{r}
# Calculate Monetary values for each customer
monetary <- Cust_Seg %>%
  dplyr::select(CustomerID, Spent) %>%
  group_by(CustomerID) %>%
  summarize(Spent = sum(Spent, na.rm = TRUE))

# Display the first few rows of Monetary values
head(monetary,3)


```

4. RFM Complete 
```{r}
# Inner join the three RFM data frames by CustomerID
rfm_table <- recency %>% 
  inner_join(df_frequency, by = "CustomerID") %>% 
  inner_join(monetary, by = "CustomerID")

# Drop the days from recency column and transform it into numeric data type
rfm_table <- rfm_table %>% 
  mutate(recency = str_replace(recency, " days", "")) %>% 
  mutate(recency = as.numeric(recency)) %>% 
  ungroup()

head(rfm_table, 3)

```


```{r}
# Deleting the CustomerID column to have only our 3 RFM features for our modelling data frame
rfm_cleaned <- select(rfm_table, -CustomerID)
```


```{r}
h1 <- hchart(
  rfm_cleaned$recency, 
  color = "#9ea4b9", name = "Recency")

h2 <- hchart(
  rfm_cleaned$frequency, 
  color = "#ff5a36", name = "Frequency")

h3 <- hchart(
  rfm_cleaned$Spent, 
  color = "#bd559c", name = "Monetary Value")

htmltools::browsable(hw_grid(h1, h2, h3, ncol = 3, rowheight = 500))

```
<h3>**K-means Clustering - **</h3>
<h4> Scaling Data: </h4>

```{r}
# scaling
rfm_norm <- scale(rfm_cleaned)
summary(rfm_norm)
```

<h4> Number of Clusters: </h4>
1. Elbow Method: 
```{r}
# Computing Using Elbow Method
fviz_nbclust(
  rfm_cleaned, 
  kmeans, 
  method = "wss"
) +
  geom_vline(xintercept = 3, linetype = 2) +
  labs(subtitle = "Elbow Method")
```
2. Silhouette Method

```{r}
#Computing Using Silhouette Method
fviz_nbclust(
  rfm_norm, 
  kmeans, 
  method = "silhouette"
) +
  labs(subtitle = "Silhouette Method")
```
3. Gap Statistic Method

```{r}
#Computing Using Gap Statistic Method
suppressWarnings({
  gap_stat <- clusGap(
    rfm_norm, 
    FUN = kmeans, 
    nstart = 25,
    K.max = 10, 
    B = 50
  )
})

fviz_gap_stat(gap_stat) + labs(subtitle = "Gap statistic method")

```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```

